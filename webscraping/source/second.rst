Daily top subreddit post CLI
****************************

The goal
--------

Build a CLI tool to get the 10 most popular posts of the day on a particular subreddit and save them to a CSV file.

The setup
---------

The setup is exactly the same as in :ref:`first-setup` in the first task, so you can just create another file in your
current project, and remember to create a *Run Configuration* for it as well to make for easier debugging!

The pieces
----------

Reddit is a great source of content, whether you're looking for news, humor or pictures of felines. Imagine working at
an office which has blocked the access to Reddit, so you want to build a tool which quickly gets the top 10 links from
your favorite subreddit before you head out to work.

As before, to save time, I've done the exploring work for you (Which, in my opinion, is one of my favorite things in a
webscraping project, but, as I said before, at this point, we definitely don't have a lot of time left)!

So, first things first - I want to bring your attention to one of my favorite packages in the standard library - :code:`argparse`.

It provides a high-level interface to :code:`sys.argv` for creating CLI tools with autogenerated help, argument validation
and all that good stuff! With `extensive documentation <https://docs.python.org/3/library/argparse.html>`__!

Here's the boilerplate to get you started:

.. code-block:: python

    import argparse

    parser = argparse.ArgumentParser(
        description='Get the top 10 links of the day from a subreddit.'
    )

    parser.add_argument(
        'subreddit',
         help='A subreddit on Reddit',
         type=str
    )

    args = parser.parse_args()
    subreddit = args.subreddit

You'll notice that if you run the script without any arguments, :code:`argparse` will display an error message, and if you
invoke it with :code:`-h`, it will display a nice, autogenerated help message!

Now to the actual Reddit part.

How do we get a a link to the top posts of a subreddit? If you're not familiar with Reddit, the format is *https://www.reddit.com/somesubredditname/top*,
so you need to dynamically build a link, to perform a request.

What if a subreddit doesn't exist. Traditionally, you'd get a 404 response, but Reddit redirects you to a search. Of course,
since the subreddit doesn't exist, the search will come up empty, so you need to think of a way to find out if your request
led you to an empty search page.

You might notice that Reddit might not give you the responses you might expect. At one point it'll start giving you :code:`HTTP 429 Too Many Requests`,
and tell you that you're a bot. That's quite insulting. You're a human, not a bot. We don't take insults from a internet
forum (we occasionally do from it's residents, but that's a different story). So what do we do? Do we walk away accepting defeat?
No, we pretend we're an internet browser, by spoofing the User Agent string. Here's the code to save you the time of looking
it up yourself:

.. code-block:: python

    response = requests.get(subreddit_url, headers={
         # As an insult to injury, we're pretending to be a harmless Windows user.
         'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:58.0) Gecko/20100101 Firefox/58.0'
    })

When that's done and you're successfully getting HTML from subreddits, you need to query all the items in a list (hint:
they all have the CSS class :code:`.thing`) and loop over them.

Within a post, getting the post metadata is quite simple - to get the title, you just need to find the first link within it.
The post title is the text node of the :code:`a` element, and the post link is, of course, the :code:`href` attribute.
You can get the upvote count it's a bit trickier - there are three :code:`div` tags with the class :code:`.score`, since Reddit
dynamically hides them, depending on if you've upvoted, downvoted or haven't voted on the entry at all - you need the second one,
which is the current value (since the bot isn't logged in, none of the posts will be downvoted or upvoted, so the second one
will always be the correct score.

If you feel like doing some exploratory work yourself, you can try and find out, how to get, let's say, the author and the
comment count!

Now the final piece of the puzzle - writing it to CSV. You need a destination where to write the file - you can either
hardcode the path to the final CSV, like the current user's Document folder, or, you can add an argument to your :code:`ArgumentParser`
to let the user supply the output path. But, be aware, it's error prone, so you should be prepared for handling exceptions.

As for generating the actual CSV, you don't have to do anything yourself - there's the :code:`csv` library for that!
The `documentation <https://docs.python.org/3/library/csv.html>`__ is great with a lot of examples. Here's a basic one
to get the ball rolling:

.. code-block:: python

    with open(path_to_destination, 'w', newline='') as file_obj:
        writer = csv.writer(file_obj)
        writer.writerow(['first column', 'second column', 'third column'])
        writer.writerow(['foo', 'bar', 'baz'])

That should be all you need to get this thing to a finish line! Good luck! Feel free to call for me if you need assistance!

In conclusion
-------------

Since we live in a day and age where the most popular public platforms, including reddit provide REST APIs for their data,
it's not really necessary to take the webscraping approach, but, at least now you have the knowledge to build handy CLI
tools for your daily automated computing needs, and you know how to create CSV files programmatically with Python, therefore
if you decide to join Nester's data-analysis workshop, you have the knowledge to create your own datasets :)

Solution
--------

Just like with the previous task, here is a heavily commented, but this time - a class-based :download:`solution <second.py>`
for this task!
